# knative-serving

![Version: 1.0.1](https://img.shields.io/badge/Version-1.0.1-informational?style=flat-square) ![Type: application](https://img.shields.io/badge/Type-application-informational?style=flat-square) ![AppVersion: 1.0.1](https://img.shields.io/badge/AppVersion-1.0.1-informational?style=flat-square)

A Helm chart for Knative Serving on Kubernetes

## Maintainers

| Name | Email | Url |
| ---- | ------ | --- |
| wangyifei | <wangyifei@kubesphere.io> |  |

## Source Code

* <https://github.com/knative/serving>

## Prerequisites

* Kubernetes cluster with RBAC (Role-Based Access Control) enabled is required
* Helm 3.4.0 or newer

## Resources Required
* <https://knative.dev/docs/install/yaml-install/serving/install-serving-with-yaml/#prerequisites>

## Install the Chart

Ensure Helm is initialized in your Kubernetes cluster.

For more details on initializing Helm, [read the Helm docs](https://helm.sh/docs/)

1. Add openfunction.github.io as an helm repo
    ```
    helm repo add openfunction https://openfunction.github.io/helm-charts/
    helm repo update
    ```

2. Install the knative-serving chart on your cluster in the knative-serving namespace:
    ```
    helm install knative-serving openfunction/knative-serving -n knative-serving --create-namespace
    ```

## Verify installation

```
kubectl get pods -namespace knative-serving
```

## Uninstall the Chart

To uninstall/delete the `knative-serving` release:
```
helm uninstall knative-serving -n knative-serving
```

## Values

| Key | Type | Default | Description |
|-----|------|---------|-------------|
| ScaleKourierGateway.kourierGateway.image.repository | string | `"docker.io/envoyproxy/envoy"` |  |
| ScaleKourierGateway.kourierGateway.image.tag | string | `"v1.17-latest"` |  |
| activator.activator.image.digest | string | `"sha256:ca607f73e5daef7f3db0358e145220f8423e93c20ee7ea9f5595f13bd508289a"` |  |
| activator.activator.image.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/activator"` |  |
| activator.activator.image.tag | string | `nil` |  |
| activator.activator.resources.limits.cpu | string | `"1"` |  |
| activator.activator.resources.limits.memory | string | `"600Mi"` |  |
| activator.activator.resources.requests.cpu | string | `"300m"` |  |
| activator.activator.resources.requests.memory | string | `"60Mi"` |  |
| activatorService.ports[0].name | string | `"http-metrics"` |  |
| activatorService.ports[0].port | int | `9090` |  |
| activatorService.ports[0].targetPort | int | `9090` |  |
| activatorService.ports[1].name | string | `"http-profiling"` |  |
| activatorService.ports[1].port | int | `8008` |  |
| activatorService.ports[1].targetPort | int | `8008` |  |
| activatorService.ports[2].name | string | `"http"` |  |
| activatorService.ports[2].port | int | `80` |  |
| activatorService.ports[2].targetPort | int | `8012` |  |
| activatorService.ports[3].name | string | `"http2"` |  |
| activatorService.ports[3].port | int | `81` |  |
| activatorService.ports[3].targetPort | int | `8013` |  |
| activatorService.type | string | `"ClusterIP"` |  |
| autoscaler.autoscaler.image.digest | string | `"sha256:31aed8b5b241147585cb0e48366451a96354fc6036d6a5667997237c1d302d98"` |  |
| autoscaler.autoscaler.image.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/autoscaler"` |  |
| autoscaler.autoscaler.image.tag | string | `nil` |  |
| autoscaler.autoscaler.resources.limits.cpu | string | `"1"` |  |
| autoscaler.autoscaler.resources.limits.memory | string | `"1000Mi"` |  |
| autoscaler.autoscaler.resources.requests.cpu | string | `"100m"` |  |
| autoscaler.autoscaler.resources.requests.memory | string | `"100Mi"` |  |
| autoscaler.ports[0].name | string | `"http-metrics"` |  |
| autoscaler.ports[0].port | int | `9090` |  |
| autoscaler.ports[0].targetPort | int | `9090` |  |
| autoscaler.ports[1].name | string | `"http-profiling"` |  |
| autoscaler.ports[1].port | int | `8008` |  |
| autoscaler.ports[1].targetPort | int | `8008` |  |
| autoscaler.ports[2].name | string | `"http"` |  |
| autoscaler.ports[2].port | int | `8080` |  |
| autoscaler.ports[2].targetPort | int | `8080` |  |
| autoscaler.replicas | int | `1` |  |
| autoscaler.type | string | `"ClusterIP"` |  |
| configAutoscaler.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# The Revision ContainerConcurrency field specifies the maximum number\n# of requests the Container can handle at once. Container concurrency\n# target percentage is how much of that maximum to use in a stable\n# state. E.g. if a Revision specifies ContainerConcurrency of 10, then\n# the Autoscaler will try to maintain 7 concurrent connections per pod\n# on average.\n# Note: this limit will be applied to container concurrency set at every\n# level (ConfigMap, Revision Spec or Annotation).\n# For legacy and backwards compatibility reasons, this value also accepts\n# fractional values in (0, 1] interval (i.e. 0.7 â‡’ 70%).\n# Thus minimal percentage value must be greater than 1.0, or it will be\n# treated as a fraction.\n# NOTE: that this value does not affect actual number of concurrent requests\n#       the user container may receive, but only the average number of requests\n#       that the revision pods will receive.\ncontainer-concurrency-target-percentage: \"70\"\n\n# The container concurrency target default is what the Autoscaler will\n# try to maintain when concurrency is used as the scaling metric for the\n# Revision and the Revision specifies unlimited concurrency.\n# When revision explicitly specifies container concurrency, that value\n# will be used as a scaling target for autoscaler.\n# When specifying unlimited concurrency, the autoscaler will\n# horizontally scale the application based on this target concurrency.\n# This is what we call \"soft limit\" in the documentation, i.e. it only\n# affects number of pods and does not affect the number of requests\n# individual pod processes.\n# The value must be a positive number such that the value multiplied\n# by container-concurrency-target-percentage is greater than 0.01.\n# NOTE: that this value will be adjusted by application of\n#       container-concurrency-target-percentage, i.e. by default\n#       the system will target on average 70 concurrent requests\n#       per revision pod.\n# NOTE: Only one metric can be used for autoscaling a Revision.\ncontainer-concurrency-target-default: \"100\"\n\n# The requests per second (RPS) target default is what the Autoscaler will\n# try to maintain when RPS is used as the scaling metric for a Revision and\n# the Revision specifies unlimited RPS. Even when specifying unlimited RPS,\n# the autoscaler will horizontally scale the application based on this\n# target RPS.\n# Must be greater than 1.0.\n# NOTE: Only one metric can be used for autoscaling a Revision.\nrequests-per-second-target-default: \"200\"\n\n# The target burst capacity specifies the size of burst in concurrent\n# requests that the system operator expects the system will receive.\n# Autoscaler will try to protect the system from queueing by introducing\n# Activator in the request path if the current spare capacity of the\n# service is less than this setting.\n# If this setting is 0, then Activator will be in the request path only\n# when the revision is scaled to 0.\n# If this setting is > 0 and container-concurrency-target-percentage is\n# 100% or 1.0, then activator will always be in the request path.\n# -1 denotes unlimited target-burst-capacity and activator will always\n# be in the request path.\n# Other negative values are invalid.\ntarget-burst-capacity: \"200\"\n\n# When operating in a stable mode, the autoscaler operates on the\n# average concurrency over the stable window.\n# Stable window must be in whole seconds.\nstable-window: \"60s\"\n\n# When observed average concurrency during the panic window reaches\n# panic-threshold-percentage the target concurrency, the autoscaler\n# enters panic mode. When operating in panic mode, the autoscaler\n# scales on the average concurrency over the panic window which is\n# panic-window-percentage of the stable-window.\n# Must be in the [1, 100] range.\n# When computing the panic window it will be rounded to the closest\n# whole second, at least 1s.\npanic-window-percentage: \"10.0\"\n\n# The percentage of the container concurrency target at which to\n# enter panic mode when reached within the panic window.\npanic-threshold-percentage: \"200.0\"\n\n# Max scale up rate limits the rate at which the autoscaler will\n# increase pod count. It is the maximum ratio of desired pods versus\n# observed pods.\n# Cannot be less or equal to 1.\n# I.e with value of 2.0 the number of pods can at most go N to 2N\n# over single Autoscaler period (2s), but at least N to\n# N+1, if Autoscaler needs to scale up.\nmax-scale-up-rate: \"1000.0\"\n\n# Max scale down rate limits the rate at which the autoscaler will\n# decrease pod count. It is the maximum ratio of observed pods versus\n# desired pods.\n# Cannot be less or equal to 1.\n# I.e. with value of 2.0 the number of pods can at most go N to N/2\n# over single Autoscaler evaluation period (2s), but at\n# least N to N-1, if Autoscaler needs to scale down.\nmax-scale-down-rate: \"2.0\"\n\n# Scale to zero feature flag.\nenable-scale-to-zero: \"true\"\n\n# Scale to zero grace period is the time an inactive revision is left\n# running before it is scaled to zero (must be positive, but recommended\n# at least a few seconds if running with mesh networking).\n# This is the upper limit and is provided not to enforce timeout after\n# the revision stopped receiving requests for stable window, but to\n# ensure network reprogramming to put activator in the path has completed.\n# If the system determines that a shorter period is satisfactory,\n# then the system will only wait that amount of time before scaling to 0.\n# NOTE: this period might actually be 0, if activator has been\n# in the request path sufficiently long.\n# If there is necessity for the last pod to linger longer use\n# scale-to-zero-pod-retention-period flag.\nscale-to-zero-grace-period: \"30s\"\n\n# Scale to zero pod retention period defines the minimum amount\n# of time the last pod will remain after Autoscaler has decided to\n# scale to zero.\n# This flag is for the situations where the pod startup is very expensive\n# and the traffic is bursty (requiring smaller windows for fast action),\n# but patchy.\n# The larger of this flag and `scale-to-zero-grace-period` will effectively\n# determine how the last pod will hang around.\nscale-to-zero-pod-retention-period: \"0s\"\n\n# pod-autoscaler-class specifies the default pod autoscaler class\n# that should be used if none is specified. If omitted, the Knative\n# Horizontal Pod Autoscaler (KPA) is used by default.\npod-autoscaler-class: \"kpa.autoscaling.knative.dev\"\n\n# The capacity of a single activator task.\n# The `unit` is one concurrent request proxied by the activator.\n# activator-capacity must be at least 1.\n# This value is used for computation of the Activator subset size.\n# See the algorithm here: http://bit.ly/38XiCZ3.\n# TODO(vagababov): tune after actual benchmarking.\nactivator-capacity: \"100.0\"\n\n# initial-scale is the cluster-wide default value for the initial target\n# scale of a revision after creation, unless overridden by the\n# \"autoscaling.knative.dev/initialScale\" annotation.\n# This value must be greater than 0 unless allow-zero-initial-scale is true.\ninitial-scale: \"1\"\n\n# allow-zero-initial-scale controls whether either the cluster-wide initial-scale flag,\n# or the \"autoscaling.knative.dev/initialScale\" annotation, can be set to 0.\nallow-zero-initial-scale: \"false\"\n\n# max-scale is the cluster-wide default value for the max scale of a revision,\n# unless overridden by the \"autoscaling.knative.dev/maxScale\" annotation.\n# If set to 0, the revision has no maximum scale.\nmax-scale: \"0\"\n\n# scale-down-delay is the amount of time that must pass at reduced\n# concurrency before a scale down decision is applied. This can be useful,\n# for example, to maintain replica count and avoid a cold start penalty if\n# more requests come in within the scale down delay period.\n# The default, 0s, imposes no delay at all.\nscale-down-delay: \"0s\"\n\n# max-scale-limit sets the maximum permitted value for the max scale of a revision.\n# When this is set to a positive value, a revision with a maxScale above that value\n# (including a maxScale of \"0\" = unlimited) is disallowed.\n# A value of zero (the default) allows any limit, including unlimited.\nmax-scale-limit: \"0\"\n"` |  |
| configDefaults.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# revision-timeout-seconds contains the default number of\n# seconds to use for the revision's per-request timeout, if\n# none is specified.\nrevision-timeout-seconds: \"300\"  # 5 minutes\n\n# max-revision-timeout-seconds contains the maximum number of\n# seconds that can be used for revision-timeout-seconds.\n# This value must be greater than or equal to revision-timeout-seconds.\n# If omitted, the system default is used (600 seconds).\n#\n# If this value is increased, the activator's terminationGraceTimeSeconds\n# should also be increased to prevent in-flight requests being disrupted.\nmax-revision-timeout-seconds: \"600\"  # 10 minutes\n\n# revision-cpu-request contains the cpu allocation to assign\n# to revisions by default.  If omitted, no value is specified\n# and the system default is used.\n# Below is an example of setting revision-cpu-request.\n# By default, it is not set by Knative.\nrevision-cpu-request: \"400m\"  # 0.4 of a CPU (aka 400 milli-CPU)\n\n# revision-memory-request contains the memory allocation to assign\n# to revisions by default.  If omitted, no value is specified\n# and the system default is used.\n# Below is an example of setting revision-memory-request.\n# By default, it is not set by Knative.\nrevision-memory-request: \"100M\"  # 100 megabytes of memory\n\n# revision-ephemeral-storage-request contains the ephemeral storage\n# allocation to assign to revisions by default.  If omitted, no value is\n# specified and the system default is used.\nrevision-ephemeral-storage-request: \"500M\"  # 500 megabytes of storage\n\n# revision-cpu-limit contains the cpu allocation to limit\n# revisions to by default.  If omitted, no value is specified\n# and the system default is used.\n# Below is an example of setting revision-cpu-limit.\n# By default, it is not set by Knative.\nrevision-cpu-limit: \"1000m\"  # 1 CPU (aka 1000 milli-CPU)\n\n# revision-memory-limit contains the memory allocation to limit\n# revisions to by default.  If omitted, no value is specified\n# and the system default is used.\n# Below is an example of setting revision-memory-limit.\n# By default, it is not set by Knative.\nrevision-memory-limit: \"200M\"  # 200 megabytes of memory\n\n# revision-ephemeral-storage-limit contains the ephemeral storage\n# allocation to limit revisions to by default.  If omitted, no value is\n# specified and the system default is used.\nrevision-ephemeral-storage-limit: \"750M\"  # 750 megabytes of storage\n\n# container-name-template contains a template for the default\n# container name, if none is specified.  This field supports\n# Go templating and is supplied with the ObjectMeta of the\n# enclosing Service or Configuration, so values such as\n# {{.Name}} are also valid.\ncontainer-name-template: \"user-container\"\n\n# container-concurrency specifies the maximum number\n# of requests the Container can handle at once, and requests\n# above this threshold are queued.  Setting a value of zero\n# disables this throttling and lets through as many requests as\n# the pod receives.\ncontainer-concurrency: \"0\"\n\n# The container concurrency max limit is an operator setting ensuring that\n# the individual revisions cannot have arbitrary large concurrency\n# values, or autoscaling targets. `container-concurrency` default setting\n# must be at or below this value.\n#\n# Must be greater than 1.\n#\n# Note: even with this set, a user can choose a containerConcurrency\n# of 0 (i.e. unbounded) unless allow-container-concurrency-zero is\n# set to \"false\".\ncontainer-concurrency-max-limit: \"1000\"\n\n# allow-container-concurrency-zero controls whether users can\n# specify 0 (i.e. unbounded) for containerConcurrency.\nallow-container-concurrency-zero: \"true\"\n\n# enable-service-links specifies the default value used for the\n# enableServiceLinks field of the PodSpec, when it is omitted by the user.\n# See: https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/#accessing-the-service\n#\n# This is a tri-state flag with possible values of (true|false|default).\n#\n# In environments with large number of services it is suggested\n# to set this value to `false`.\n# See https://github.com/knative/serving/issues/8498.\nenable-service-links: \"false\"\n"` |  |
| configDeployment.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# List of repositories for which tag to digest resolving should be skipped\nregistries-skipping-tag-resolving: \"kind.local,ko.local,dev.local\"\n\n# Maximum time allowed for an image's digests to be resolved.\ndigest-resolution-timeout: \"10s\"\n\n# Duration we wait for the deployment to be ready before considering it failed.\nprogress-deadline: \"600s\"\n\n# Sets the queue proxy's CPU request.\n# If omitted, a default value (currently \"25m\"), is used.\nqueue-sidecar-cpu-request: \"25m\"\n\n# Sets the queue proxy's CPU limit.\n# If omitted, no value is specified and the system default is used.\nqueue-sidecar-cpu-limit: \"1000m\"\n\n# Sets the queue proxy's memory request.\n# If omitted, no value is specified and the system default is used.\nqueue-sidecar-memory-request: \"400Mi\"\n\n# Sets the queue proxy's memory limit.\n# If omitted, no value is specified and the system default is used.\nqueue-sidecar-memory-limit: \"800Mi\"\n\n# Sets the queue proxy's ephemeral storage request.\n# If omitted, no value is specified and the system default is used.\nqueue-sidecar-ephemeral-storage-request: \"512Mi\"\n\n# Sets the queue proxy's ephemeral storage limit.\n# If omitted, no value is specified and the system default is used.\nqueue-sidecar-ephemeral-storage-limit: \"1024Mi\"\n\n# The freezer service endpoint that queue-proxy calls when its traffic drops to zero or\n# scales up from zero.\n#\n# Freezer service is available at: https://github.com/knative-sandbox/container-freezer\n# or users may write their own service.\n#\n# The value will need to include both the host and the port that will be accessed.\n# For the host, $HOST_IP can be passed, and the appropriate host IP value will be swapped\n# in at runtime, which will enable the freezer daemonset to be reachable via the node IP.\n#\n# As an example:\n#     concurrency-state-endpoint: \"http://$HOST_IP:9696\"\n#\n# If not set, queue proxy takes no action (this is the default behavior).\n#\n# When enabled, a serviceAccountToken will be mounted to queue-proxy using\n# a projected volume. This requires the Service Account Token Volume Projection feature\n# to be enabled. For details, see this link:\n# https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/#service-account-token-volume-projection\n#\n# NOTE THAT THIS IS AN EXPERIMENTAL / ALPHA FEATURE\nconcurrency-state-endpoint: \"\""` |  |
| configDeployment.queueSidecarImage.digest | string | `"sha256:80dfb4568e08e43093f93b2cae9401f815efcb67ad8442d1f7f4c8a41e071fbe"` |  |
| configDeployment.queueSidecarImage.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/queue"` |  |
| configDeployment.queueSidecarImage.tag | string | `nil` |  |
| configDomain.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# Default value for domain.\n# Although it will match all routes, it is the least-specific rule so it\n# will only be used if no other domain matches.\nexample.com: |\n\n# These are example settings of domain.\n# example.org will be used for routes having app=nonprofit.\nexample.org: |\n  selector:\n    app: nonprofit\n\n# Routes having the cluster domain suffix (by default 'svc.cluster.local')\n# will not be exposed through Ingress. You can define your own label\n# selector to assign that domain suffix to your Route here, or you can set\n# the label\n#    \"networking.knative.dev/visibility=cluster-local\"\n# to achieve the same effect.  This shows how to make routes having\n# the label app=secret only exposed to the local cluster.\nsvc.cluster.local: |\n  selector:\n    app: secret\n"` |  |
| configFeatures.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# Indicates whether multi container support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#multi-containers\nmulti-container: \"enabled\"\n\n# Indicates whether Kubernetes affinity support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-node-affinity\nkubernetes.podspec-affinity: \"disabled\"\n\n# Indicates whether Kubernetes hostAliases support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-host-aliases\nkubernetes.podspec-hostaliases: \"disabled\"\n\n# Indicates whether Kubernetes nodeSelector support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-node-selector\nkubernetes.podspec-nodeselector: \"disabled\"\n\n# Indicates whether Kubernetes tolerations support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-toleration\nkubernetes.podspec-tolerations: \"disabled\"\n\n# Indicates whether Kubernetes FieldRef support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-fieldref\nkubernetes.podspec-fieldref: \"enabled\"\n\n# Indicates whether Kubernetes RuntimeClassName support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-runtime-class\nkubernetes.podspec-runtimeclassname: \"disabled\"\n\n# This feature allows end-users to set a subset of fields on the Pod's SecurityContext\n#\n# When set to \"enabled\" or \"allowed\" it allows the following\n# PodSecurityContext properties:\n# - FSGroup\n# - RunAsGroup\n# - RunAsNonRoot\n# - SupplementalGroups\n# - RunAsUser\n#\n# This feature flag should be used with caution as the PodSecurityContext\n# properties may have a side-effect on non-user sidecar containers that come\n# from Knative or your service mesh\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-security-context\nkubernetes.podspec-securitycontext: \"disabled\"\n\n# Indicates whether Kubernetes PriorityClassName support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-priority-class-name\nkubernetes.podspec-priorityclassname: \"disabled\"\n\n# Indicates whether Kubernetes SchedulerName support is enabled\n#\n# WARNING: Cannot safely be disabled once enabled.\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-scheduler-name\nkubernetes.podspec-schedulername: \"disabled\"\n\n# This feature flag allows end-users to add a subset of capabilities on the Pod's SecurityContext.\n#\n# When set to \"enabled\" or \"allowed\" it allows capabilities to be added to the container.\n# For a list of possible capabilities, see https://man7.org/linux/man-pages/man7/capabilities.7.html\nkubernetes.containerspec-addcapabilities: \"disabled\"\n\n# This feature validates PodSpecs from the validating webhook\n# against the K8s API Server.\n#\n# When \"enabled\", the server will always run the extra validation.\n# When \"allowed\", the server will not run the dry-run validation by default.\n#   However, clients may enable the behavior on an individual Service by\n#   attaching the following metadata annotation: \"features.knative.dev/podspec-dryrun\":\"enabled\".\n# See: https://knative.dev/docs/serving/feature-flags/#kubernetes-dry-run\nkubernetes.podspec-dryrun: \"allowed\"\n\n# Controls whether tag header based routing feature are enabled or not.\n# 1. Enabled: enabling tag header based routing\n# 2. Disabled: disabling tag header based routing\n# See: https://knative.dev/docs/serving/feature-flags/#tag-header-based-routing\ntag-header-based-routing: \"disabled\"\n\n# Controls whether http2 auto-detection should be enabled or not.\n# 1. Enabled: http2 connection will be attempted via upgrade.\n# 2. Disabled: http2 connection will only be attempted when port name is set to \"h2c\".\nautodetect-http2: \"disabled\"\n\n# Controls whether volume support for EmptyDir is enabled or not.\n# 1. Enabled: enabling EmptyDir volume support\n# 2. Disabled: disabling EmptyDir volume support\nkubernetes.podspec-volumes-emptydir: \"disabled\"\n\n# Controls whether init containers support is enabled or not.\n# 1. Enabled: enabling init containers support\n# 2. Disabled: disabling init containers support\nkubernetes.podspec-init-containers: \"disabled\""` |  |
| configGc.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n\n# ---------------------------------------\n# Garbage Collector Settings\n# ---------------------------------------\n#\n# Active\n#   * Revisions which are referenced by a Route are considered active.\n#   * Individual revisions may be marked with the annotation\n#      \"serving.knative.dev/no-gc\":\"true\" to be permanently considered active.\n#   * Active revisions are not considered for GC.\n# Retention\n#   * Revisions are retained if they are any of the following:\n#       1. Active\n#       2. Were created within \"retain-since-create-time\"\n#       3. Were last referenced by a route within\n#           \"retain-since-last-active-time\"\n#       4. There are fewer than \"min-non-active-revisions\"\n#     If none of these conditions are met, or if the count of revisions exceed\n#      \"max-non-active-revisions\", they will be deleted by GC.\n#     The special value \"disabled\" may be used to turn off these limits.\n#\n# Example config to immediately collect any inactive revision:\n#    min-non-active-revisions: \"0\"\n#    retain-since-create-time: \"disabled\"\n#    retain-since-last-active-time: \"disabled\"\n#\n# Example config to always keep around the last ten non-active revisions:\n#     retain-since-create-time: \"disabled\"\n#     retain-since-last-active-time: \"disabled\"\n#     max-non-active-revisions: \"10\"\n#\n# Example config to disable all GC:\n#     retain-since-create-time: \"disabled\"\n#     retain-since-last-active-time: \"disabled\"\n#     max-non-active-revisions: \"disabled\"\n#\n# Example config to keep recently deployed or active revisions,\n# always maintain the last two in case of rollback, and prevent\n# burst activity from exploding the count of old revisions:\n#      retain-since-create-time: \"48h\"\n#      retain-since-last-active-time: \"15h\"\n#      min-non-active-revisions: \"2\"\n#      max-non-active-revisions: \"1000\"\n\n# Duration since creation before considering a revision for GC or \"disabled\".\nretain-since-create-time: \"48h\"\n\n# Duration since active before considering a revision for GC or \"disabled\".\nretain-since-last-active-time: \"15h\"\n\n# Minimum number of non-active revisions to retain.\nmin-non-active-revisions: \"20\"\n\n# Maximum number of non-active revisions to retain\n# or \"disabled\" to disable any maximum limit.\nmax-non-active-revisions: \"1000\"\n"` |  |
| configKourier.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# Specifies whether requests reaching the Kourier gateway\n# in the context of services should be logged. Readiness\n# probes etc. must be configured via the bootstrap config.\nenable-service-access-logging: \"true\"\n"` |  |
| configLeaderElection.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# lease-duration is how long non-leaders will wait to try to acquire the\n# lock; 15 seconds is the value used by core kubernetes controllers.\nlease-duration: \"15s\"\n\n# renew-deadline is how long a leader will try to renew the lease before\n# giving up; 10 seconds is the value used by core kubernetes controllers.\nrenew-deadline: \"10s\"\n\n# retry-period is how long the leader election client waits between tries of\n# actions; 2 seconds is the value used by core kubernetes controllers.\nretry-period: \"2s\"\n\n# buckets is the number of buckets used to partition key space of each\n# Reconciler. If this number is M and the replica number of the controller\n# is N, the N replicas will compete for the M buckets. The owner of a\n# bucket will take care of the reconciling for the keys partitioned into\n# that bucket.\nbuckets: \"1\"\n"` |  |
| configLogging.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# Common configuration for all Knative codebase\nzap-logger-config: |\n  {\n    \"level\": \"info\",\n    \"development\": false,\n    \"outputPaths\": [\"stdout\"],\n    \"errorOutputPaths\": [\"stderr\"],\n    \"encoding\": \"json\",\n    \"encoderConfig\": {\n      \"timeKey\": \"timestamp\",\n      \"levelKey\": \"severity\",\n      \"nameKey\": \"logger\",\n      \"callerKey\": \"caller\",\n      \"messageKey\": \"message\",\n      \"stacktraceKey\": \"stacktrace\",\n      \"lineEnding\": \"\",\n      \"levelEncoder\": \"\",\n      \"timeEncoder\": \"iso8601\",\n      \"durationEncoder\": \"\",\n      \"callerEncoder\": \"\"\n    }\n  }\n\n# Log level overrides\n# For all components except the queue proxy,\n# changes are picked up immediately.\n# For queue proxy, changes require recreation of the pods.\nloglevel.controller: \"info\"\nloglevel.autoscaler: \"info\"\nloglevel.queueproxy: \"info\"\nloglevel.webhook: \"info\"\nloglevel.activator: \"info\"\nloglevel.hpaautoscaler: \"info\"\nloglevel.net-certmanager-controller: \"info\"\nloglevel.net-istio-controller: \"info\"\n"` |  |
| configNetwork.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# ingress-class specifies the default ingress class\n# to use when not dictated by Route annotation.\n#\n# If not specified, will use the Istio ingress.\n#\n# Note that changing the Ingress class of an existing Route\n# will result in undefined behavior.  Therefore it is best to only\n# update this value during the setup of Knative, to avoid getting\n# undefined behavior.\ningress-class: \"kourier.ingress.networking.knative.dev\"\n\n# certificate-class specifies the default Certificate class\n# to use when not dictated by Route annotation.\n#\n# If not specified, will use the Cert-Manager Certificate.\n#\n# Note that changing the Certificate class of an existing Route\n# will result in undefined behavior.  Therefore it is best to only\n# update this value during the setup of Knative, to avoid getting\n# undefined behavior.\ncertificate-class: \"cert-manager.certificate.networking.knative.dev\"\n\n# namespace-wildcard-cert-selector specifies a LabelSelector which\n# determines which namespaces should have a wildcard certificate\n# provisioned.\n#\n# Use an empty value to disable the feature (this is the default):\n#   namespace-wildcard-cert-selector: \"\"\n#\n# Use an empty object to enable for all namespaces\n#   namespace-wildcard-cert-selector: {}\n#\n# Useful labels include the \"kubernetes.io/metadata.name\" label to\n# avoid provisioning a certifcate for the \"kube-system\" namespaces.\n# Use the following selector to match pre-1.0 behavior of using\n# \"networking.knative.dev/disableWildcardCert\" to exclude namespaces:\n#\n# matchExpressions:\n# - key: \"networking.knative.dev/disableWildcardCert\"\n#   operator: \"NotIn\"\n#   values: [\"true\"]\nnamespace-wildcard-cert-selector: \"\"\n\n# domain-template specifies the golang text template string to use\n# when constructing the Knative service's DNS name. The default\n# value is \"{{.Name}}.{{.Namespace}}.{{.Domain}}\".\n#\n# Valid variables defined in the template include Name, Namespace, Domain,\n# Labels, and Annotations. Name will be the result of the tagTemplate\n# below, if a tag is specified for the route.\n#\n# Changing this value might be necessary when the extra levels in\n# the domain name generated is problematic for wildcard certificates\n# that only support a single level of domain name added to the\n# certificate's domain. In those cases you might consider using a value\n# of \"{{.Name}}-{{.Namespace}}.{{.Domain}}\", or removing the Namespace\n# entirely from the template. When choosing a new value be thoughtful\n# of the potential for conflicts - for example, when users choose to use\n# characters such as `-` in their service, or namespace, names.\n# {{.Annotations}} or {{.Labels}} can be used for any customization in the\n# go template if needed.\n# We strongly recommend keeping namespace part of the template to avoid\n# domain name clashes:\n# eg. '{{.Name}}-{{.Namespace}}.{{ index .Annotations \"sub\"}}.{{.Domain}}'\n# and you have an annotation {\"sub\":\"foo\"}, then the generated template\n# would be {Name}-{Namespace}.foo.{Domain}\ndomain-template: \"{{.Name}}.{{.Namespace}}.{{.Domain}}\"\n\n# tagTemplate specifies the golang text template string to use\n# when constructing the DNS name for \"tags\" within the traffic blocks\n# of Routes and Configuration.  This is used in conjunction with the\n# domainTemplate above to determine the full URL for the tag.\ntag-template: \"{{.Tag}}-{{.Name}}\"\n\n# Controls whether TLS certificates are automatically provisioned and\n# installed in the Knative ingress to terminate external TLS connection.\n# 1. Enabled: enabling auto-TLS feature.\n# 2. Disabled: disabling auto-TLS feature.\nauto-tls: \"Disabled\"\n\n# Controls the behavior of the HTTP endpoint for the Knative ingress.\n# It requires autoTLS to be enabled.\n# 1. Enabled: The Knative ingress will be able to serve HTTP connection.\n# 2. Redirected: The Knative ingress will send a 301 redirect for all\n# http connections, asking the clients to use HTTPS.\n#\n# \"Disabled\" option is deprecated.\nhttp-protocol: \"Enabled\"\n\n# rollout-duration contains the minimal duration in seconds over which the\n# Configuration traffic targets are rolled out to the newest revision.\nrollout-duration: \"0\"\n\n# autocreate-cluster-domain-claims controls whether ClusterDomainClaims should\n# be automatically created (and deleted) as needed when DomainMappings are\n# reconciled.\n#\n# If this is \"false\" (the default), the cluster administrator is\n# responsible for creating ClusterDomainClaims and delegating them to\n# namespaces via their spec.Namespace field. This setting should be used in\n# multitenant environments which need to control which namespace can use a\n# particular domain name in a domain mapping.\n#\n# If this is \"true\", users are able to associate arbitrary names with their\n# services via the DomainMapping feature.\nautocreate-cluster-domain-claims: \"false\"\n\n# If true, networking plugins can add additional information to deployed\n# applications to make their pods directly accessible via their IPs even if mesh is\n# enabled and thus direct-addressability is usually not possible.\n# Consumers like Knative Serving can use this setting to adjust their behavior\n# accordingly, i.e. to drop fallback solutions for non-pod-addressable systems.\n#\n# NOTE: This flag is in an alpha state and is mostly here to enable internal testing\n#       for now. Use with caution.\nenable-mesh-pod-addressability: \"false\"\n\n# mesh-compatibility-mode indicates whether consumers of network plugins\n# should directly contact Pod IPs (most efficient), or should use the\n# Cluster IP (less efficient, needed when mesh is enabled unless\n# `enable-mesh-pod-addressability`, above, is set).\n# Permitted values are:\n#  - \"auto\" (default): automatically determine which mesh mode to use by trying Pod IP and falling back to Cluster IP as needed.\n#  - \"enabled\": always use Cluster IP and do not attempt to use Pod IPs.\n#  - \"disabled\": always use Pod IPs and do not fall back to Cluster IP on failure.\nmesh-compatibility-mode: \"auto\"\n\n# Defines the scheme used for external URLs if autoTLS is not enabled.\n# This can be used for making Knative report all URLs as \"HTTPS\" for example, if you're\n# fronting Knative with an external loadbalancer that deals with TLS termination and\n# Knative doesn't know about that otherwise.\ndefault-external-scheme: \"http\"\n"` |  |
| configObservability.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n\n# logging.enable-var-log-collection defaults to false.\n# The fluentd daemon set will be set up to collect /var/log if\n# this flag is true.\nlogging.enable-var-log-collection: \"false\"\n\n# logging.revision-url-template provides a template to use for producing the\n# logging URL that is injected into the status of each Revision.\nlogging.revision-url-template: \"http://logging.example.com/?revisionUID=${REVISION_UID}\"\n\n# If non-empty, this enables queue proxy writing user request logs to stdout, excluding probe\n# requests.\n# NB: after 0.18 release logging.enable-request-log must be explicitly set to true\n# in order for request logging to be enabled.\n#\n# The value determines the shape of the request logs and it must be a valid go text/template.\n# It is important to keep this as a single line. Multiple lines are parsed as separate entities\n# by most collection agents and will split the request logs into multiple records.\n#\n# The following fields and functions are available to the template:\n#\n# Request: An http.Request (see https://golang.org/pkg/net/http/#Request)\n# representing an HTTP request received by the server.\n#\n# Response:\n# struct {\n#   Code    int       // HTTP status code (see https://www.iana.org/assignments/http-status-codes/http-status-codes.xhtml)\n#   Size    int       // An int representing the size of the response.\n#   Latency float64   // A float64 representing the latency of the response in seconds.\n# }\n#\n# Revision:\n# struct {\n#   Name          string  // Knative revision name\n#   Namespace     string  // Knative revision namespace\n#   Service       string  // Knative service name\n#   Configuration string  // Knative configuration name\n#   PodName       string  // Name of the pod hosting the revision\n#   PodIP         string  // IP of the pod hosting the revision\n# }\n#\nlogging.request-log-template: '{\"httpRequest\": {\"requestMethod\": \"{{.Request.Method}}\", \"requestUrl\": \"{{js .Request.RequestURI}}\", \"requestSize\": \"{{.Request.ContentLength}}\", \"status\": {{.Response.Code}}, \"responseSize\": \"{{.Response.Size}}\", \"userAgent\": \"{{js .Request.UserAgent}}\", \"remoteIp\": \"{{js .Request.RemoteAddr}}\", \"serverIp\": \"{{.Revision.PodIP}}\", \"referer\": \"{{js .Request.Referer}}\", \"latency\": \"{{.Response.Latency}}s\", \"protocol\": \"{{.Request.Proto}}\"}, \"traceId\": \"{{index .Request.Header \"X-B3-Traceid\"}}\"}'\n\n# If true, the request logging will be enabled.\n# NB: up to and including Knative version 0.18 if logging.request-log-template is non-empty, this value\n# will be ignored.\nlogging.enable-request-log: \"false\"\n\n# If true, this enables queue proxy writing request logs for probe requests to stdout.\n# It uses the same template for user requests, i.e. logging.request-log-template.\nlogging.enable-probe-request-log: \"false\"\n\n# metrics.backend-destination field specifies the system metrics destination.\n# It supports either prometheus (the default) or opencensus.\nmetrics.backend-destination: prometheus\n\n# metrics.request-metrics-backend-destination specifies the request metrics\n# destination. It enables queue proxy to send request metrics.\n# Currently supported values: prometheus (the default), opencensus.\nmetrics.request-metrics-backend-destination: prometheus\n\n# profiling.enable indicates whether it is allowed to retrieve runtime profiling data from\n# the pods via an HTTP server in the format expected by the pprof visualization tool. When\n# enabled, the Knative Serving pods expose the profiling data on an alternate HTTP port 8008.\n# The HTTP context root for profiling is then /debug/pprof/.\nprofiling.enable: \"false\"\n"` |  |
| configTracing.Example | string | `"################################\n#                              #\n#    EXAMPLE CONFIGURATION     #\n#                              #\n################################\n\n# This block is not actually functional configuration,\n# but serves to illustrate the available configuration\n# options and document them in a way that is accessible\n# to users that `kubectl edit` this config map.\n#\n# These sample configuration options may be copied out of\n# this example block and unindented to be in the data block\n# to actually change the configuration.\n#\n# This may be \"zipkin\" or \"none\" (default)\nbackend: \"none\"\n\n# URL to zipkin collector where traces are sent.\n# This must be specified when backend is \"zipkin\"\nzipkin-endpoint: \"http://zipkin.istio-system.svc.cluster.local:9411/api/v2/spans\"\n\n# Enable zipkin debug mode. This allows all spans to be sent to the server\n# bypassing sampling.\ndebug: \"false\"\n\n# Percentage (0-1) of requests to trace\nsample-rate: \"0.1\"\n"` |  |
| controller.controller.image.digest | string | `"sha256:c5a77d5642065ff3452d9b043a7226b85bfc81dc068f8dded905abf88d917a4d"` |  |
| controller.controller.image.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/controller"` |  |
| controller.controller.image.tag | string | `nil` |  |
| controller.controller.resources.limits.cpu | string | `"1"` |  |
| controller.controller.resources.limits.memory | string | `"1000Mi"` |  |
| controller.controller.resources.requests.cpu | string | `"100m"` |  |
| controller.controller.resources.requests.memory | string | `"100Mi"` |  |
| controller.ports[0].name | string | `"http-metrics"` |  |
| controller.ports[0].port | int | `9090` |  |
| controller.ports[0].targetPort | int | `9090` |  |
| controller.ports[1].name | string | `"http-profiling"` |  |
| controller.ports[1].port | int | `8008` |  |
| controller.ports[1].targetPort | int | `8008` |  |
| controller.type | string | `"ClusterIP"` |  |
| defaultDomain.job.image.digest | string | `"sha256:4bd6f5a7748644e56bb266d5c10f442b2548b484b57059ee64ec2a793dd1b976"` |  |
| defaultDomain.job.image.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/default-domain"` |  |
| defaultDomain.job.image.tag | string | `nil` |  |
| defaultDomain.ports[0].name | string | `"http"` |  |
| defaultDomain.ports[0].port | int | `80` |  |
| defaultDomain.ports[0].targetPort | int | `8080` |  |
| defaultDomain.type | string | `"ClusterIP"` |  |
| domainMapping.domainMapping.image.digest | string | `"sha256:6b5356cf3a2b64d52cbbf1bc0de376b816c4d3f610ccc8ff2dabf3d259c0996b"` |  |
| domainMapping.domainMapping.image.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping"` |  |
| domainMapping.domainMapping.image.tag | string | `nil` |  |
| domainMapping.domainMapping.resources.limits.cpu | string | `"300m"` |  |
| domainMapping.domainMapping.resources.limits.memory | string | `"400Mi"` |  |
| domainMapping.domainMapping.resources.requests.cpu | string | `"30m"` |  |
| domainMapping.domainMapping.resources.requests.memory | string | `"40Mi"` |  |
| domainmappingWebhook.domainmappingWebhook.image.digest | string | `"sha256:d0cc86f2002660c4804f6e0aed8218d39384c73a8b5006c9ac558becd8f48aa6"` |  |
| domainmappingWebhook.domainmappingWebhook.image.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/domain-mapping-webhook"` |  |
| domainmappingWebhook.domainmappingWebhook.image.tag | string | `nil` |  |
| domainmappingWebhook.domainmappingWebhook.resources.limits.cpu | string | `"500m"` |  |
| domainmappingWebhook.domainmappingWebhook.resources.limits.memory | string | `"500Mi"` |  |
| domainmappingWebhook.domainmappingWebhook.resources.requests.cpu | string | `"100m"` |  |
| domainmappingWebhook.domainmappingWebhook.resources.requests.memory | string | `"100Mi"` |  |
| domainmappingWebhook.ports[0].name | string | `"http-metrics"` |  |
| domainmappingWebhook.ports[0].port | int | `9090` |  |
| domainmappingWebhook.ports[0].targetPort | int | `9090` |  |
| domainmappingWebhook.ports[1].name | string | `"http-profiling"` |  |
| domainmappingWebhook.ports[1].port | int | `8008` |  |
| domainmappingWebhook.ports[1].targetPort | int | `8008` |  |
| domainmappingWebhook.ports[2].name | string | `"https-webhook"` |  |
| domainmappingWebhook.ports[2].port | int | `443` |  |
| domainmappingWebhook.ports[2].targetPort | int | `8443` |  |
| domainmappingWebhook.type | string | `"ClusterIP"` |  |
| kourier.ports[0].name | string | `"http2"` |  |
| kourier.ports[0].port | int | `80` |  |
| kourier.ports[0].protocol | string | `"TCP"` |  |
| kourier.ports[0].targetPort | int | `8080` |  |
| kourier.ports[1].name | string | `"https"` |  |
| kourier.ports[1].port | int | `443` |  |
| kourier.ports[1].protocol | string | `"TCP"` |  |
| kourier.ports[1].targetPort | int | `8443` |  |
| kourier.type | string | `"LoadBalancer"` |  |
| kourierBootstrap.envoyBootstrapYaml.admin.accessLogPath | string | `"/dev/stdout"` |  |
| kourierBootstrap.envoyBootstrapYaml.admin.address.pipe.path | string | `"/tmp/envoy.admin"` |  |
| kourierBootstrap.envoyBootstrapYaml.dynamicResources.adsConfig.apiType | string | `"GRPC"` |  |
| kourierBootstrap.envoyBootstrapYaml.dynamicResources.adsConfig.rateLimitSettings | object | `{}` |  |
| kourierBootstrap.envoyBootstrapYaml.dynamicResources.adsConfig.transportApiVersion | string | `"V3"` |  |
| kourierBootstrap.envoyBootstrapYaml.dynamicResources.cdsConfig.ads | object | `{}` |  |
| kourierBootstrap.envoyBootstrapYaml.dynamicResources.cdsConfig.resourceApiVersion | string | `"V3"` |  |
| kourierBootstrap.envoyBootstrapYaml.dynamicResources.ldsConfig.ads | object | `{}` |  |
| kourierBootstrap.envoyBootstrapYaml.dynamicResources.ldsConfig.resourceApiVersion | string | `"V3"` |  |
| kourierBootstrap.envoyBootstrapYaml.node.cluster | string | `"kourier-knative"` |  |
| kourierBootstrap.envoyBootstrapYaml.node.id | string | `"3scale-kourier-gateway"` |  |
| kourierInternal.ports[0].name | string | `"http2"` |  |
| kourierInternal.ports[0].port | int | `80` |  |
| kourierInternal.ports[0].protocol | string | `"TCP"` |  |
| kourierInternal.ports[0].targetPort | int | `8081` |  |
| kourierInternal.type | string | `"ClusterIP"` |  |
| kubernetesClusterDomain | string | `"cluster.local"` |  |
| netKourierController.controller.image.digest | string | `"sha256:cd70f2bb54f2575082e54b0df1b74908c3f4873ffc9750ca8da36d7b9bfe5b2d"` |  |
| netKourierController.controller.image.repository | string | `"gcr.io/knative-releases/knative.dev/net-kourier/cmd/kourier"` |  |
| netKourierController.controller.image.tag | string | `nil` |  |
| netKourierController.ports[0].name | string | `"grpc-xds"` |  |
| netKourierController.ports[0].port | int | `18000` |  |
| netKourierController.ports[0].protocol | string | `"TCP"` |  |
| netKourierController.ports[0].targetPort | int | `18000` |  |
| netKourierController.replicas | int | `1` |  |
| netKourierController.type | string | `"ClusterIP"` |  |
| webhook.ports[0].name | string | `"http-metrics"` |  |
| webhook.ports[0].port | int | `9090` |  |
| webhook.ports[0].targetPort | int | `9090` |  |
| webhook.ports[1].name | string | `"http-profiling"` |  |
| webhook.ports[1].port | int | `8008` |  |
| webhook.ports[1].targetPort | int | `8008` |  |
| webhook.ports[2].name | string | `"https-webhook"` |  |
| webhook.ports[2].port | int | `443` |  |
| webhook.ports[2].targetPort | int | `8443` |  |
| webhook.type | string | `"ClusterIP"` |  |
| webhook.webhook.image.digest | string | `"sha256:bd954ec8ced56e359bd4f60ee1886b20000df14126688c796383a3ae40cfffc0"` |  |
| webhook.webhook.image.repository | string | `"gcr.io/knative-releases/knative.dev/serving/cmd/webhook"` |  |
| webhook.webhook.image.tag | string | `nil` |  |
| webhook.webhook.resources.limits.cpu | string | `"500m"` |  |
| webhook.webhook.resources.limits.memory | string | `"500Mi"` |  |
| webhook.webhook.resources.requests.cpu | string | `"100m"` |  |
| webhook.webhook.resources.requests.memory | string | `"100Mi"` |  |

----------------------------------------------
Autogenerated from chart metadata using [helm-docs v1.10.0](https://github.com/norwoodj/helm-docs/releases/v1.10.0)
